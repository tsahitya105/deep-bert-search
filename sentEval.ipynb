{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\user\\anaconda3\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: h5py in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (1.20.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\anaconda3\\lib\\site-packages (from keras) (5.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from h5py->keras) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached https://files.pythonhosted.org/packages/15/89/b785d557e3c806abc8beaae664571d71e8c4eb736a2c32b69aba9932cbd1/gensim-4.0.1-cp37-cp37m-win_amd64.whl\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.20.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: Cython==0.29.21 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (0.29.21)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Installing collected packages: gensim\n",
      "  Found existing installation: gensim 3.8.1\n",
      "    Uninstalling gensim-3.8.1:\n",
      "      Successfully uninstalled gensim-3.8.1\n",
      "Successfully installed gensim-4.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: sentence-transformers in c:\\users\\user\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.20.2)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.8.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.61.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: torchvision in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.9.1)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.1.95)\n",
      "Requirement already satisfied, skipping upgrade: huggingface-hub in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.0.13)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.22.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.4.5)\n",
      "Requirement already satisfied, skipping upgrade: transformers<5.0.0,>=4.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.8.2)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers) (6.2.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub->sentence-transformers) (20.9)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub->sentence-transformers) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub->sentence-transformers) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (0.13.2)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.4.4)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.45)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from packaging>=20.9->huggingface-hub->sentence-transformers) (2.4.2)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->sentence-transformers) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->sentence-transformers) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->sentence-transformers) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->huggingface-hub->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->huggingface-hub->sentence-transformers) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in c:\\users\\user\\anaconda3\\lib\\site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->huggingface-hub->sentence-transformers) (7.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/SentEval\n",
      "  Cloning https://github.com/facebookresearch/SentEval to c:\\users\\user\\appdata\\local\\temp\\pip-req-build-y_wtji8k\n",
      "Requirement already satisfied (use --upgrade to upgrade): SentEval==0.1.0 from git+https://github.com/facebookresearch/SentEval in c:\\users\\user\\anaconda3\\lib\\site-packages\n",
      "Building wheels for collected packages: SentEval\n",
      "  Building wheel for SentEval (setup.py): started\n",
      "  Building wheel for SentEval (setup.py): finished with status 'done'\n",
      "  Created wheel for SentEval: filename=SentEval-0.1.0-cp37-none-any.whl size=35245 sha256=a4d09b466c9e8eb02a5cb607bad74d252b78adb13fac86217007fb239705ca63\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-df59ewj3\\wheels\\12\\d1\\1e\\7e46b4b6d1b480028b6a9dec849bf801846023d4c93d172bbc\n",
      "Successfully built SentEval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n",
      "  Running command git clone -q https://github.com/facebookresearch/SentEval 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-req-build-y_wtji8k'\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install gensim -U\n",
    "!pip install nltk\n",
    "!pip install -U sentence-transformers\n",
    "!pip install git+https://github.com/facebookresearch/SentEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "import gensim\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import io\n",
    "from gensim.models import KeyedVectors as evalVec\n",
    "FIN_PATH = 'fin-how2-ted.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import senteval\n",
    "ft_model = gensim.models.Word2Vec.load(\"ft2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"sentences.txt\"\n",
    "dataset=[]\n",
    "with open(filepath,encoding=\"utf8\") as fp:\n",
    "    for cnt, line in enumerate(fp):\n",
    "        dataset.append(line)\n",
    "dataset=dataset[:5000]      \n",
    "len(dataset[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SentEval prepare and batcher\n",
    "def prepare(params, samples):\n",
    "    _, params.word2id = create_dictionary(samples)\n",
    "    params.word_vec = get_wordvec(PATH_TO_VEC, params.word2id)\n",
    "    params.wvec_dim = 100\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"sent-eval-data\"\n",
    "embed_size = 100\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 10}\n",
    "params['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n",
    "                                 'tenacity': 3, 'epoch_size': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(x):\n",
    "    return np.sqrt(np.sum(x**2))\n",
    "\n",
    "def div_norm(x):\n",
    "    norm_value = l2_norm(x)\n",
    "    if norm_value > 0:\n",
    "        return x * ( 1.0 / norm_value)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def getSentenceEmbedding(sent, model):\n",
    "    sentArr=sent.split(\" \")\n",
    "    start = np.zeros(embed_size)\n",
    "    for word in sentArr:\n",
    "        word_embed=np.array([model.wv[word]])\n",
    "        start+=div_norm(word_embed)\n",
    "    return start/len(sentArr)\n",
    "\n",
    "# def batcher(params, batch):\n",
    "#     embeddings = []\n",
    "#     for sent in dataset:\n",
    "#         sentvec = getSentenceEmbedding(sent,ft_model)\n",
    "#         embeddings.append(sentvec)\n",
    "#     embeddings = np.vstack(embeddings)\n",
    "#     return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary\n",
    "def create_dictionary(sentences, threshold=0):\n",
    "    words = {}\n",
    "    for s in sentences:\n",
    "        for word in s:\n",
    "            words[word] = words.get(word, 0) + 1\n",
    "\n",
    "    if threshold > 0:\n",
    "        newwords = {}\n",
    "        for word in words:\n",
    "            if words[word] >= threshold:\n",
    "                newwords[word] = words[word]\n",
    "        words = newwords\n",
    "    words['<s>'] = 1e9 + 4\n",
    "    words['</s>'] = 1e9 + 3\n",
    "    words['<p>'] = 1e9 + 2\n",
    "\n",
    "    sorted_words = sorted(words.items(), key=lambda x: -x[1])  # inverse sort\n",
    "    id2word = []\n",
    "    word2id = {}\n",
    "    for i, (w, _) in enumerate(sorted_words):\n",
    "        id2word.append(w)\n",
    "        word2id[w] = i\n",
    "\n",
    "    return id2word, word2id\n",
    "\n",
    "# Get word vectors from vocabulary (glove, word2vec, fasttext ..)\n",
    "def get_wordvec(path_to_vec, word2id):\n",
    "    word_vec = {}\n",
    "    for wd in word2id:\n",
    "        word_vec[wd] = ft_model.wv[wd].T\n",
    "    logging.info('Found {0} words with word vectors, out of {1} words'.format(len(word_vec), len(word2id)))\n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_VEC = 'ft2.model'\n",
    "#se = senteval.engine.SE(params, batcher, prepare)\n",
    "se = evalVec.load_word2vec_format(FIN_PATH, binary=True)\n",
    "\n",
    "def batcher(params, batch):\n",
    "    batch = []\n",
    "    for x in dataset:\n",
    "        if(x!=\"\" and x!=\" \"):\n",
    "            batch.append(x.split(\" \"))\n",
    "    embeddings = []\n",
    "    for sent in batch:\n",
    "        sentvec = []\n",
    "        for word in sent:\n",
    "            if word in params.word_vec:\n",
    "                sentvec.append(params.word_vec[word])\n",
    "        if not sentvec:\n",
    "            vec = np.zeros(params.wvec_dim)\n",
    "            sentvec.append(vec)\n",
    "        sentvec = np.mean(sentvec, 0)\n",
    "        embeddings.append(sentvec)\n",
    "\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.6238773466616108, 1.7963237724170263e-39),\n",
       " SpearmanrResult(correlation=0.6589215888009288, pvalue=2.5346056459149263e-45),\n",
       " 0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se.evaluate_word_pairs('eval/wordsim353.tsv',dummy4unknown=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.448323335546951, 1.480562485841737e-50),\n",
       " SpearmanrResult(correlation=0.4371714088057533, pvalue=6.926977701395097e-48),\n",
       " 0.10010010010010009)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se.evaluate_word_pairs('eval/simlex-999.tsv',dummy4unknown=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
